# -*- coding: utf-8 -*-
"""Text Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V26jl6sJAgQU_gBObaDDBGPhCqkcIBo3
"""

!pip install nltk

!pip install Sastrawi

# Contoh teks
teks_asli = "Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase."

# Mengubah teks menjadi lowercase
teks_lowercase = teks_asli.lower()

# Menampilkan hasil
print("Teks asli:", teks_asli)
print("Teks setelah diubah menjadi lowercase:", teks_lowercase)

# Fungsi untuk menghapus angka dari teks
def hapus_angka(teks):
    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])
    return teks_tanpa_angka

# Contoh teks dengan angka
teks_dengan_angka = "Ini adalah contoh teks dengan angka 12345 yang akan dihapus."

# Memanggil fungsi untuk menghapus angka
teks_tanpa_angka = hapus_angka(teks_dengan_angka)

# Menampilkan hasil
print("Teks dengan angka:", teks_dengan_angka)
print("Teks tanpa angka:", teks_tanpa_angka)

import re

def hapus_angka_tidak_relevan(teks):
    # Menggunakan regex untuk mengidentifikasi dan menghapus angka yang tidak relevan
    # Pola untuk mengenali angka yang harus dihapus, termasuk nomor rumah dan nomor telepon
    pola_angka_tidak_relevan = r"\b(?:\d{1,3}[-\.\s]?)?(?:\d{3}[-\.\s]?)?\d{4,}\b"
    hasil = re.sub(pola_angka_tidak_relevan, "", teks)
    return hasil.strip()

# Contoh kalimat dengan angka
kalimat = "Di sini ada 3 nomor rumah yaitu  123, 456, dan 789. Silakan hubungi 081234567890 untuk informasi lebih lanjut."

# Memanggil fungsi untuk menghapus angka tidak relevan
hasil_tanpa_angka = hapus_angka_tidak_relevan(kalimat)

# Menampilkan hasil
print("Kalimat dengan angka:", kalimat)
print("Kalimat tanpa angka tidak relevan:", hasil_tanpa_angka)

import string

def remove_punctuation(text):
    # Membuat set yang berisi semua tanda baca
    punctuation_set = set(string.punctuation)

    # Menghapus tanda baca dari teks
    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)

    return text_without_punctuation

# Contoh teks dengan tanda baca
teks_asli = "Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi."

# Menghapus tanda baca dari teks
teks_tanpa_tanda_baca = remove_punctuation(teks_asli)

# Menampilkan hasil
print("Teks asli:", teks_asli)
print("Teks setelah menghapus tanda baca:", teks_tanpa_tanda_baca)

teks = "   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    "
teks_setelah_strip = teks.strip()
print(teks_setelah_strip)

teks_dengan_whitespace = "Ini adalah    contoh kalimat    dengan spasi    di dalamnya."
teks_tanpa_whitespace = teks_dengan_whitespace.replace(" ", "")
print(teks_tanpa_whitespace)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh
nltk.download('stopwords')
nltk.download('punkt_tab')  # Untuk tokenisasi kata

teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."

# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)

# Ambil daftar stopwords bahasa Indonesia dari NLTK
stopwords_indonesia = set(stopwords.words('indonesian'))

# Filtering kata-kata dengan menghapus stopwords
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]

# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)

print("Teks asli:", teks)
print("Teks setelah filtering stopwords NLTK:", teks_tanpa_stopwords)

# Install Sastrawi library if not already installed
!pip install Sastrawi

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from nltk.tokenize import word_tokenize

# Inisialisasi objek StopWordRemover dari Sastrawi
factory = StopWordRemoverFactory()
stopwords_sastrawi = factory.get_stop_words()

teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."

# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)

# Filtering kata-kata dengan menghapus stopwords Sastrawi
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]

# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)

print("Teks asli:", teks)
print("Teks setelah filtering stopwords Sastrawi:", teks_tanpa_stopwords)

from nltk.tokenize import word_tokenize

text = "Ini adalah contoh tokenisasi kata dalam pemrosesan teks."
tokens = word_tokenize(text)
print(tokens)

from nltk.tokenize import sent_tokenize

text = "Ini adalah contoh tokenisasi kalimat. Apakah ini kalimat kedua? Ya, ini kalimat ketiga!"
sentences = sent_tokenize(text)
print(sentences)

from nltk.tokenize import TreebankWordTokenizer
# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)
text = "Pemrosesan teks adalah cabang ilmu komputer yang berfokus pada pengolahan teks dan dokumen."
tokenizer = TreebankWordTokenizer()
phrases = tokenizer.tokenize(text)
print(phrases)

# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia
import re

text = "Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan."
tokens = re.findall(r'\w+|\d+', text)
print(tokens)

# Misalnya menggunakan spasi sebagai pemisah kata
text = "Ini adalah contoh tokenisasi berbasis model."
tokens = text.split()
print(tokens)

import nltk
from nltk.stem import PorterStemmer

# Inisialisasi stemmer
stemmer = PorterStemmer()

# Kata-kata asli
words = ["running", "runs", "runner", "ran", "easily", "fairness", "better", "best", "cats", "cacti", "geese", "rocks", "oxen"]

# Melakukan stemming pada setiap kata
for word in words:
    stemmed_word = stemmer.stem(word)
    print(f"Kata asli: {word}, Kata setelah stemming: {stemmed_word}")

import nltk
from nltk.stem import WordNetLemmatizer

# Download wordnet jika belum di-download
nltk.download('wordnet')

# Inisialisasi lemmatizer
lemmatizer = WordNetLemmatizer()

# Kata-kata asli
words = ["Run", "Cat", "Good", "Goose", "Rock", "City", "Big", "Happy", "Run", "Sleep"]

# Melakukan lematisasi pada setiap kata
for word in words:
    lemma_word = lemmatizer.lemmatize(word.lower())  # Mengonversi ke huruf kecil untuk memastikan pemrosesan yang konsisten
    print(f"Kata asli: {word}, Kata setelah lematisasi: {lemma_word}")