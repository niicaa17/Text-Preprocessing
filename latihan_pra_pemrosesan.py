# -*- coding: utf-8 -*-
"""Latihan Pra-pemrosesan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GqzLBMdfSvi7LRUgSvqFWKvnR3p1MRMY
"""

# Contoh teks
teks_asli = "Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase."

# Mengubah teks menjadi lowercase
teks_lowercase = teks_asli.lower()

# Menampilkan hasil
print("Teks asli:", teks_asli)
print("Teks setelah diubah menjadi lowercase:", teks_lowercase)

# Contoh teks dengan campuran huruf besar dan kecil
teks_asli = """
Ini adalah contoh teks dengan campuran huruf besar dan kecil.
Contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.
Dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.
Ini membantu dalam memastikan konsistensi dalam analisis teks.
"""

# Mengubah teks menjadi lowercase menggunakan case folding
teks_case_folded = teks_asli.lower()

# Menampilkan hasil
print("Teks asli:")
print(teks_asli)
print("Teks setelah case folding:")
print(teks_case_folded)

# Fungsi untuk menghapus angka dari teks
def hapus_angka(teks):
    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])
    return teks_tanpa_angka

# Contoh teks dengan angka
teks_dengan_angka = "Ini adalah contoh teks dengan angka 12345 yang akan dihapus."

# Memanggil fungsi untuk menghapus angka
teks_tanpa_angka = hapus_angka(teks_dengan_angka)

# Menampilkan hasil
print("Teks dengan angka:", teks_dengan_angka)
print("Teks tanpa angka:", teks_tanpa_angka)

import string

def remove_punctuation(text):
    # Membuat set yang berisi semua tanda baca
    punctuation_set = set(string.punctuation)

    # Menghapus tanda baca dari teks
    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)

    return text_without_punctuation

# Contoh teks dengan banyak tanda baca
teks_asli = """
Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar. Kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan. Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!
"""

# Menghapus tanda baca dari teks
teks_tanpa_tanda_baca = remove_punctuation(teks_asli)

# Menampilkan hasil
print("Teks asli:")
print(teks_asli)
print("\nTeks setelah menghapus tanda baca:")

teks = "   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    "
teks_setelah_strip = teks.strip()
print(teks_setelah_strip)

teks_dengan_whitespace = "Ini adalah    contoh kalimat    dengan spasi    di dalamnya."
teks_tanpa_whitespace = teks_dengan_whitespace.replace(" ", "")
print(teks_tanpa_whitespace)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh
nltk.download('stopwords')
nltk.download('punkt_tab')  # Untuk tokenisasi kata

teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."

# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)

# Ambil daftar stopwords bahasa Indonesia dari NLTK
stopwords_indonesia = set(stopwords.words('indonesian'))

# Filtering kata-kata dengan menghapus stopwords
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]

# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)

print("Teks asli:", teks)
print("Teks setelah filtering stopwords NLTK:", teks_tanpa_stopwords)

# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)
text = "Ini adalah contoh kalimat untuk tokenisasi kata"
phrases = text.split(' ')
print(phrases)

# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia
import re

text = "Ini adalah contoh kalimat pertama. Dan ini adalah contoh kalimat kedua."
sentences = re.split(r'(?<=[.!?]) +', text)
sentences = re.split(r'(?<=[.!?]) +', text)

# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)
text = "Apel, jeruk, pisang, dan mangga."
phrases = text.split(',')
print(phrases)

# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia
import re

text = "Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan."
tokens = re.findall(r'\w+|\d+', text)
print(tokens)

# Misalnya menggunakan spasi sebagai pemisah kata
text = "Ini adalah contoh tokenisasi berbasis model."
tokens = text.split()
print(tokens)

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
words = ["running", "easily", "bought", "crying", "leaves"]
stemmed_words = [stemmer.stem(word) for word in words]
print(stemmed_words)

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

lemmatizer = WordNetLemmatizer()
words = ["running", "easily", "bought", "crying", "leaves"]

lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]
print(lemmatized_words)